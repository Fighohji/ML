

## 激活函数

- Sigmoid Function

    用于逼近分段线性函数

    $y=c\frac{1}{1 + e^{-(b+wx_i)}}=c*sigmoid(b+wx_i)$

![image-20240531145802078](C:\Users\Fighoh\Pictures\image-20240531145802078.png)

- Rectified Linear Unit(ReLU)

    $y=c*max(0, b+wx_i)$

    由两个ReLU可以组合成一个线性分段函数

- Swish 激活函数的定义如下：

  $Swish(x)=x⋅σ(x)$

  其中，$\sigma(x) $是$ Sigmoid $函数$=\frac{1}{1+e^{−x}}$

## Optimization fails

- critical point(临界点，微分为0的点)
  - local minimal
  - saddle point(鞍点)

## Tayler Series Approximation（泰勒级数近似）

- 用于估计函数点附近的值
- $L(\theta)\approx L(\theta')+(\theta-\theta ')^Tg+\frac{1}{2}(\theta-\theta')H(\theta-\theta')$

- $g = \nabla L(\theta'),\quad g_i=\frac{\partial L(\theta')}{\partial\theta_i}$
- $H_{ij}=\frac{\partial L(\theta')}{\partial\theta_i\partial\theta_j}$

- 当到达critical附近，$(\theta-\theta ')^Tg=0$
- 令$v=(\theta-\theta ')$
  - $\forall v,v^THv > 0\rightarrow Local\ minimal$
  - $\forall v,v^THv < 0\rightarrow Local\ maximal$
  - $\exist v,v^THv > 0\ \and\ \exist v,v^THv < 0 \rightarrow Saddle\ point$

## Soft-max

$$
y_i' = \frac{exp(y_i)}{\sum_{j}{exp(y_i)}}
$$

- Softmax是一种常用的数学函数，通常用于多分类问题中的机器学习和深度学习模型中。它将一组输入（通常是模型的输出，即logits）转换为概率分布，表示每个类的概率值
- 只有两个标签时用sigmoid即可，此时两者等价
- 因为在二分类问题中我们通常只需要考虑一个logit $z$，另一个logit可以设置为0

## Loss Function

- MSE (Regression)
  - $e = \sum\limits_{i}(\widehat {y_i}-y_i')^2$

- Cross-entropy (交叉熵)(Classification)
  - $e=-\sum\limits_i\widehat{y_i}\ln{y_i'}$
  - pytorch中使用Cross-entropy会自动在net最后加入一层softmax

## optimizer

- Adam（Adaptive Moment Estimation）
- RAdam（Rectified Adam）
- 使用：`optimizer = torch.optim.RAdam(model.parameters(), lr=0.0001, weight_decay=1e-5)`
